{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCH-USD\n",
      "BTC-USD\n",
      "ETH-USD\n",
      "LTC-USD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CRISTIAN CHAVEZ\\AppData\\Local\\Temp\\ipykernel_27544\\441976918.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  main_df.fillna(method=\"ffill\", inplace=True)  # Rellena los huecos de los datos usando los valores conocidos previos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Longitud de la secuencia precedente para recolectar para la RNN\n",
    "SEQ_LEN = 60\n",
    "\n",
    "# Cuánto tiempo en el futuro estamos tratando de predecir\n",
    "FUTURE_PERIOD_PREDICT = 8\n",
    "\n",
    "# El ratio que queremos predecir\n",
    "RATIO_TO_PREDICT = \"BCH-USD\"\n",
    "\n",
    "EPOCHS = 10  # cuántas pasadas a través de nuestros datos\n",
    "BATCH_SIZE = 64  # ¿cuántos lotes? Intenta con un tamaño de lote más pequeño si estás recibiendo errores OOM (fuera de memoria).\n",
    "NAME = f\"{RATIO_TO_PREDICT}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # un nombre único para el modelo\n",
    "\n",
    "# Función para clasificar si el valor futuro es mayor que el valor actual\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop([\"future\"], axis=1)  # Elimina la columna \"future\" que ya no necesitamos.\n",
    "\n",
    "    for col in df.columns:  # Recorre todas las columnas\n",
    "        if col != \"target\":  # Normaliza todas las columnas excepto el objetivo mismo\n",
    "            df[col] = df[col].pct_change()  # Calcula el cambio porcentual (pct_change) de cada columna\n",
    "            df.dropna(inplace=True)  # Elimina las filas con valores nulos generados por pct_change\n",
    "            df[col] = preprocessing.scale(df[col].values)  # Escala los valores entre 0 y 1.\n",
    "\n",
    "    df.dropna(inplace=True)  # Limpieza adicional para asegurarse de no tener valores nulos\n",
    "    sequential_data = []  # Lista que contendrá las secuencias\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # Estas serán nuestras secuencias reales. Se crean con deque, que mantiene la longitud máxima.\n",
    "\n",
    "    for i in df.values:  # Itera sobre los valores\n",
    "        prev_days.append([n for n in i[:-1]])  # Almacena todo menos el objetivo\n",
    "        if len(prev_days) == SEQ_LEN:  # Asegura que tengamos 60 secuencias\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # ¡Añade esas secuencias!\n",
    "\n",
    "    random.shuffle(sequential_data)  # Baraja las secuencias para mayor variedad.\n",
    "\n",
    "    buys = []  # Lista que almacenará nuestras secuencias de compra y objetivos\n",
    "    sells = []  # Lista que almacenará nuestras secuencias de venta y objetivos\n",
    "\n",
    "    for seq, target in sequential_data:  # Itera sobre los datos secuenciales\n",
    "        if target == 0:  # Si es \"no comprar\"\n",
    "            sells.append([seq, target])  # Agrega a la lista de ventas\n",
    "        elif target == 1:  # Si el objetivo es 1...\n",
    "            buys.append([seq, target])  # ¡Es una compra!\n",
    "\n",
    "    random.shuffle(buys)  # Baraja las compras\n",
    "    random.shuffle(sells)  # Baraja las ventas\n",
    "\n",
    "    lower = min(len(buys), len(sells))  # ¿Cuál es la longitud más corta?\n",
    "\n",
    "    buys = buys[:lower]  # Asegura que ambas listas tengan la misma longitud.\n",
    "    sells = sells[:lower]  # Asegura que ambas listas tengan la misma longitud.\n",
    "\n",
    "    sequential_data = buys + sells  # Combínalas\n",
    "    random.shuffle(sequential_data)  # Otra mezcla, para que el modelo no se confunda con una clase seguida de la otra.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # Recorre los datos secuenciales\n",
    "        X.append(seq)  # X son las secuencias\n",
    "        y.append(target)  # y son los objetivos/etiquetas (compras vs no compra)\n",
    "\n",
    "    return np.array(X), y  # Devuelve X e y, y convierte X en un array de numpy\n",
    "\n",
    "# Ruta del archivo de datos\n",
    "file_path = r\"C:\\Users\\CRISTIAN CHAVEZ\\Documents\\GitHub\\AI_Project_Deep-Learning-Tensorflow-Keras\\Modelo Para Predecir Criptomonedas\\crypto_data\\BTC-USD.csv\"\n",
    "\n",
    "# Leer el archivo inicial\n",
    "df = pd.read_csv(file_path, names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "\n",
    "# DataFrame principal para almacenar todos los datos\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "# Las 4 criptomonedas que queremos considerar\n",
    "ratios = [\"BCH-USD\", \"BTC-USD\", \"ETH-USD\", \"LTC-USD\"]\n",
    "\n",
    "# Ruta base donde se encuentran los archivos de datos\n",
    "base_path = r\"C:\\Users\\CRISTIAN CHAVEZ\\Documents\\GitHub\\AI_Project_Deep-Learning-Tensorflow-Keras\\Modelo Para Predecir Criptomonedas\\crypto_data\"\n",
    "\n",
    "# Iterar sobre cada ratio\n",
    "for ratio in ratios:\n",
    "    print(ratio)\n",
    "    \n",
    "    # Obtener la ruta completa al archivo\n",
    "    dataset = os.path.join(base_path, f\"{ratio}.csv\")\n",
    "    \n",
    "    # Leer el archivo específico\n",
    "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "    \n",
    "    # Renombrar columnas de 'close' y 'volume' para incluir el ticker\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "    \n",
    "    # Establecer 'time' como índice para poder unirlos por este tiempo compartido\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    \n",
    "    # Ignorar otras columnas además de 'close' y 'volume'\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    \n",
    "    # Si el DataFrame principal está vacío, simplemente asignar el DataFrame actual\n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        # De lo contrario, unir estos datos con el DataFrame principal\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "main_df.fillna(method=\"ffill\", inplace=True)  # Rellena los huecos de los datos usando los valores conocidos previos\n",
    "main_df.dropna(inplace=True)  # Elimina cualquier fila que aún tenga valores nulos\n",
    "\n",
    "# Crear una nueva columna 'future' que es el valor de cierre futuro desplazado por 'FUTURE_PERIOD_PREDICT'\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "\n",
    "# Crear una columna 'target' que clasifica si el valor futuro es mayor que el valor actual\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
    "\n",
    "main_df.dropna(inplace=True)  # Elimina cualquier fila que aún tenga valores nulos\n",
    "\n",
    "# Imprimir las primeras filas del DataFrame resultante\n",
    "#print(main_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534879440\n"
     ]
    }
   ],
   "source": [
    "# Obtener una lista ordenada de los valores del índice (los tiempos)\n",
    "times = sorted(main_df.index.values)\n",
    "\n",
    "# Calcular el punto de corte para el último 5% de los tiempos\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.05 * len(times))]\n",
    "\n",
    "# Imprimir el valor del tiempo que representa el inicio del último 5% del conjunto de datos\n",
    "print(last_5pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 82408 validation: 4134\n",
      "Dont buys: 41204, buys: 41204\n",
      "VALIDATION Dont buys: 2067, buys: 2067\n"
     ]
    }
   ],
   "source": [
    "# Crear el DataFrame de validación donde el índice es mayor o igual al punto de corte del último 5%\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "\n",
    "# Actualizar main_df para que contenga solo los datos hasta antes del último 5%\n",
    "main_df = main_df[(main_df.index < last_5pct)]\n",
    "\n",
    "# Preprocesar el DataFrame principal para obtener los datos de entrenamiento\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "\n",
    "# Preprocesar el DataFrame de validación para obtener los datos de validación\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "# Imprimir la cantidad de datos de entrenamiento y validación\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "\n",
    "# Imprimir la cantidad de etiquetas 'no compra' (0) y 'compra' (1) en los datos de entrenamiento\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "\n",
    "# Imprimir la cantidad de etiquetas 'no compra' (0) y 'compra' (1) en los datos de validación\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (82408, 60, 8)\n",
      "train_y shape: (82408,)\n",
      "validation_x shape: (4134, 60, 8)\n",
      "validation_y shape: (4134,)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CRISTIAN CHAVEZ\\Documents\\GitHub\\AI_Project_Deep-Learning-Tensorflow-Keras\\env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.5175 - loss: 0.7485\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54499, saving model to models/RNN_Final-01-0.545.keras\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 99ms/step - accuracy: 0.5175 - loss: 0.7485 - val_accuracy: 0.5450 - val_loss: 0.6875\n",
      "Epoch 2/10\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.5544 - loss: 0.6844\n",
      "Epoch 2: val_accuracy did not improve from 0.54499\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 176ms/step - accuracy: 0.5544 - loss: 0.6844 - val_accuracy: 0.5450 - val_loss: 0.6836\n",
      "Epoch 3/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.5765 - loss: 0.6764\n",
      "Epoch 3: val_accuracy improved from 0.54499 to 0.55878, saving model to models/RNN_Final-03-0.559.keras\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 196ms/step - accuracy: 0.5765 - loss: 0.6764 - val_accuracy: 0.5588 - val_loss: 0.6844\n",
      "Epoch 4/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5887 - loss: 0.6695\n",
      "Epoch 4: val_accuracy did not improve from 0.55878\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 99ms/step - accuracy: 0.5887 - loss: 0.6695 - val_accuracy: 0.5498 - val_loss: 0.6889\n",
      "Epoch 5/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.6212 - loss: 0.6489\n",
      "Epoch 5: val_accuracy did not improve from 0.55878\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 100ms/step - accuracy: 0.6212 - loss: 0.6489 - val_accuracy: 0.5493 - val_loss: 0.7430\n",
      "Epoch 6/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6649 - loss: 0.6110\n",
      "Epoch 6: val_accuracy did not improve from 0.55878\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 97ms/step - accuracy: 0.6649 - loss: 0.6110 - val_accuracy: 0.5544 - val_loss: 0.7720\n",
      "Epoch 7/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7154 - loss: 0.5569\n",
      "Epoch 7: val_accuracy did not improve from 0.55878\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 98ms/step - accuracy: 0.7154 - loss: 0.5569 - val_accuracy: 0.5535 - val_loss: 0.8539\n",
      "Epoch 8/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7541 - loss: 0.5055\n",
      "Epoch 8: val_accuracy did not improve from 0.55878\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 96ms/step - accuracy: 0.7541 - loss: 0.5055 - val_accuracy: 0.5375 - val_loss: 0.9702\n",
      "Epoch 9/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7824 - loss: 0.4658\n",
      "Epoch 9: val_accuracy did not improve from 0.55878\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 97ms/step - accuracy: 0.7824 - loss: 0.4658 - val_accuracy: 0.5305 - val_loss: 0.9981\n",
      "Epoch 10/10\n",
      "\u001b[1m1287/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8025 - loss: 0.4303\n",
      "Epoch 10: val_accuracy did not improve from 0.55878\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 96ms/step - accuracy: 0.8025 - loss: 0.4303 - val_accuracy: 0.5346 - val_loss: 1.0522\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, BatchNormalization, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "# Assuming train_x, train_y, validation_x, validation_y are lists\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "validation_x = np.array(validation_x)\n",
    "validation_y = np.array(validation_y)\n",
    "\n",
    "# Check the shapes of your data\n",
    "print(f'train_x shape: {train_x.shape}')\n",
    "print(f'train_y shape: {train_y.shape}')\n",
    "print(f'validation_x shape: {validation_x.shape}')\n",
    "print(f'validation_y shape: {validation_y.shape}')\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "filepath = \"models/RNN_Final-{epoch:02d}-{val_accuracy:.3f}.keras\"  # Use '.keras' extension\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")\n",
    "\n",
    "#Ejecutamos el siguiente comando tensorboard --logdir=logs/\n",
    "#en la ruta donde se tiene guardado el codigo\n",
    "#Despues de abrir nuestro localhost, se va a scalars y escribimos \\w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
