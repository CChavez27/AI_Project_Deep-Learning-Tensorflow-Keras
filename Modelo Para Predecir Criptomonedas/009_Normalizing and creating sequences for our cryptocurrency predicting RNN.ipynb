{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1160fca3-4841-4000-a9ab-2774d4e258ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC\n",
      "DOGE\n",
      "ETH\n",
      "QTU\n",
      "            BTC_close  BTC_volume  DOGE_close  DOGE_volume  ETH_close  \\\n",
      "time                                                                    \n",
      "1715356800    60793.4      15.101    0.144841     169877.0    2914.04   \n",
      "1715356860    60729.3       6.395    0.144513     245666.0    2907.25   \n",
      "1715356920    60744.5       7.272    0.144437     417310.0    2906.65   \n",
      "1715356980    60713.9       4.568    0.144207     311595.0    2904.65   \n",
      "1715357040    60683.8      18.586    0.144067     153653.0    2901.04   \n",
      "\n",
      "            ETH_volume  QTU_close  QTU_volume  \n",
      "time                                           \n",
      "1715356800        45.7      3.629      1283.8  \n",
      "1715356860        23.8      3.620        44.7  \n",
      "1715356920       997.9      3.611        20.8  \n",
      "1715356980        82.0      3.606        46.6  \n",
      "1715357040        96.0      3.609        32.8  \n",
      "            DOGE_close    future\n",
      "time                            \n",
      "1715356800    0.144841  0.144547\n",
      "1715356860    0.144513  0.144578\n",
      "1715356920    0.144437  0.144634\n",
      "1715356980    0.144207  0.144473\n",
      "1715357040    0.144067  0.145137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CRISTIAN CHAVEZ\\AppData\\Local\\Temp\\ipykernel_28164\\1677597163.py:85: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing  # pip install sklearn ... if you don't have it!\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Longitud de la secuencia precedente para recolectar para la RNN\n",
    "SEQ_LEN = 60\n",
    "\n",
    "# Cuánto tiempo en el futuro estamos tratando de predecir\n",
    "FUTURE_PERIOD_PREDICT = 8\n",
    "\n",
    "# El ratio que queremos predecir\n",
    "RATIO_TO_PREDICT = \"DOGE\"\n",
    "\n",
    "# Función para clasificar si el valor futuro es mayor que el valor actual\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop(columns=[\"future\"])  # ya no la necesitamos.\n",
    "\n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != \"target\":  # normalize all ... except for the target itself!\n",
    "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
    "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "\n",
    "    df.dropna(inplace=True)  # cleanup again... jic. Those nasty NaNs love to creep in.\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "\n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "\n",
    "# Uso de una cadena sin procesar para evitar errores de escape de unicode\n",
    "file_path = r\"C:\\Users\\CRISTIAN CHAVEZ\\Documents\\GitHub\\AI_Project_Deep-Learning-Tensorflow-Keras\\Modelo Para Predecir Criptomonedas\\criptodata\\DOGE.xlsx\"\n",
    "\n",
    "# Leer el archivo inicial\n",
    "df = pd.read_excel(file_path, names=['time', 'open', 'high', 'low', 'close', 'volume', 'usdVolume'])\n",
    "\n",
    "# DataFrame principal vacío para almacenar todos los datos\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "# Las 4 criptomonedas que queremos considerar\n",
    "ratios = [\"BTC\", \"DOGE\", \"ETH\", \"QTU\"]\n",
    "\n",
    "# Ruta base donde se encuentran los archivos de datos\n",
    "base_path = r\"C:\\Users\\CRISTIAN CHAVEZ\\Documents\\GitHub\\AI_Project_Deep-Learning-Tensorflow-Keras\\Modelo Para Predecir Criptomonedas\\criptodata\"\n",
    "\n",
    "# Iterar sobre cada ratio\n",
    "for ratio in ratios:\n",
    "    print(ratio)\n",
    "    \n",
    "    # Obtener la ruta completa al archivo\n",
    "    dataset = os.path.join(base_path, f\"{ratio}.xlsx\")\n",
    "    \n",
    "    # Leer el archivo específico\n",
    "    df = pd.read_excel(dataset, names=['time', 'open', 'high', 'low', 'close', 'volume', 'usdVolume'])\n",
    "    \n",
    "    # Renombrar columnas de 'close' y 'volume' para incluir el ticker\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "    \n",
    "    # Establecer 'time' como índice para poder unirlos por este tiempo compartido\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    \n",
    "    # Ignorar otras columnas además de 'close' y 'volume'\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    \n",
    "    # Si el DataFrame principal está vacío, simplemente asignar el DataFrame actual\n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        # De lo contrario, unir estos datos con el DataFrame principal\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
    "main_df.dropna(inplace=True)\n",
    "print(main_df.head())  # how did we do??\n",
    "\n",
    "# Crear una nueva columna 'future' que es el valor de cierre futuro desplazado por 'FUTURE_PERIOD_PREDICT'\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "\n",
    "# Crear una columna 'target' que clasifica si el valor futuro es mayor que el valor actual\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
    "\n",
    "# Imprimir las primeras filas del DataFrame resultante\n",
    "print(main_df[[f\"{RATIO_TO_PREDICT}_close\", \"future\"]].head())\n",
    "#print(main_df.head())  # Imprimir las primeras filas del DataFrame resultante para verificar la estructura y los datos cargados correctamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d3d8e24-832b-46a7-956e-46981146ca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1715438880\n"
     ]
    }
   ],
   "source": [
    "times = sorted(main_df.index.values)  # get the times\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
    "print(last_5pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a8a78c8-0ce9-4448-b501-aa8057adee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
    "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n",
    "preprocess_df(main_df)\n",
    "#train_x, train_y = preprocess_df(main_df) \n",
    "#validation_x, validation_y = preprocess_df(validation_main_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
